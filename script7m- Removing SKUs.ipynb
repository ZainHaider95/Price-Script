{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rocky-october",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from os.path import exists\n",
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings(action='once')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-front",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup Stuff (that should come from config files or database later)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698d8d9-6b58-4953-b6a4-dccf9b9da65f",
   "metadata": {},
   "source": [
    "CHANNEL: Setting up Channel Transaction Fee (%) for each Channel >> List of Dictionaries\n",
    "\n",
    "WAREHOUSES: Defining Keys and Shipping Charge Category for Each WareHouse >> List of Dictionaries\n",
    "\n",
    "get_warehouse_key(warehouse_name): Return Key of the given Warehouse (from the WAREHOUSES) - can be improved\n",
    "\n",
    "Initializing DEFAULT_TARGET_PROFIT(5%), DEFAULT_SHIP_MARKUP (12%), EXCLUDED_WAREHOUSES, PUNCTUATION_WAREHOUSES (Punctuation has to be retained in these)\n",
    "\n",
    "PARTS_AUTH_SHIPPING_MODEL: The Machine pretrained Machine Learning Model loaded from pickle\n",
    "PRICE_FILE_COLUMNS: Columns in the Price File\n",
    "\n",
    "PRICE_FILE_LOCATION\n",
    "CatSKU_CHANNELS // Need elaboration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "jewish-michael",
   "metadata": {},
   "outputs": [],
   "source": [
    "CS_TRANSACTION_FEE = 0.0101\n",
    "CHANNELS = [\n",
    "    {'name':'AP Fusion', 'channel_transaction_fee':0.08, 'target_profit': 0.09},\n",
    "\n",
    "    {'name':'PS Amazon', 'channel_transaction_fee':0.119},\n",
    "    {'name':'PS Walmart', 'channel_transaction_fee':0.125},\n",
    "    {'name':'PS Ebay', 'channel_transaction_fee':0.17},  ## Changed from 0.15 on Feb 24 2022\n",
    "    \n",
    "    {'name':'BS Amazon', 'channel_transaction_fee':0.12},\n",
    "    {'name':'BS Walmart', 'channel_transaction_fee':0.12},\n",
    "    {'name':'BS Ebay', 'channel_transaction_fee':0.12},\n",
    "    \n",
    "    {'name':'Mecka', 'channel_transaction_fee': 0.17} #Changed on 28th Sep but on Excel, Its not Linked with this.\n",
    "]\n",
    "\n",
    "WAREHOUSES = [\n",
    "    # Fully integrated warehouses\n",
    "    {'key':'C', 'name':'Brock', 'shipping':'free-ish'},\n",
    "    {'key':'D', 'name':'Dorman Direct', 'shipping':'free-ish'},\n",
    "    {'key':'J', 'name':'PFG', 'shipping':'theirs'},\n",
    "    {'key':'K', 'name':'Keystone', 'shipping':'theirs'},\n",
    "    {'key':'N', 'name':'NPW', 'shipping':'ours'},\n",
    "    {'key':'O', 'name':'Tonsa', 'shipping':'ours'},\n",
    "    {'key':'P', 'name':'Parts Auth', 'shipping':'theirs'},\n",
    "    {'key':'Y', 'name':'Motor State', 'shipping':'theirs'},\n",
    "    # Manual/FTP warehouses\n",
    "    {'key':'1', 'name':'Jante Wheel', 'shipping':'free'},\n",
    "    {'key':'2', 'name':'OE Wheels', 'shipping':'theirs'},\n",
    "    {'key':'6', 'name':'Burco Mirrors', 'shipping':'ours'},\n",
    "    {'key':'8', 'name':'Race Sport Lighting', 'shipping':'ours', 'target_profit': 0.1},\n",
    "    {'key':'9', 'name':'Sunbelt APG', 'shipping':'ours'},\n",
    "    {'key':'Z', 'name':'SimpleTire', 'shipping':'free'}\n",
    "    # Low-volume, or unused warehouses\n",
    "    #{'key':'5', 'name':'KSI Trading'},\n",
    "    #{'key':'7', 'name':'NTW'},\n",
    "    #{'key':'H', 'name':'Hanson'},\n",
    "    #{'key':'3', 'name':'Motor State'},\n",
    "]\n",
    "\n",
    "def get_warehouse_key(warehouse_name):\n",
    "    for warehouse in WAREHOUSES:\n",
    "        if warehouse['name'] == warehouse_name:\n",
    "            return warehouse['key']\n",
    "    return None\n",
    "\n",
    "\n",
    "DEFAULT_TARGET_PROFIT = 0.05\n",
    "DEFAULT_SHIP_MARKUP = 1 / 1.12\n",
    "EXCLUDED_WAREHOUSES = ['A','5','H','3', 'O', '7']\n",
    "PUNCTUATION_WAREHOUSES = ['J','1','C', '9', '8', 'Y', 'F', '7']\n",
    "\n",
    "PARTS_AUTH_SHIPPING_MODEL = 'shipping-research/tree-model.pkl'\n",
    "\n",
    "PRICE_FILE_COLUMNS = ['CS-SKU-NP', 'MinPrice', 'Shipping', 'Carrier', 'Service', 'Markup',\n",
    "       'ShipMkup', 'ListMkup', 'PackQty', 'MinQty', 'MaxQty', 'Zip Code',\n",
    "       'CatSKU', 'OP-Lowest(Y)', 'VND-Lowest(Y)', 'MinMkDown', 'MaxMkUp', 'Interval',\n",
    "       'BundleSKU', 'Duplicate']\n",
    "\n",
    "PRICE_FILE_LOCATION = 'price-files'\n",
    "\n",
    "CatSKU_CHANNELS = ['PS Ebay']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-archive",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-retro",
   "metadata": {},
   "source": [
    "#### Load in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e732700-6cb3-4b1c-9e4b-06541d7e0297",
   "metadata": {},
   "source": [
    "Reading the Price Weight file from Disk\n",
    "\n",
    "warehouses: DataFrame containing unique warehouse keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "committed-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = pd.read_csv('inventory/pw-all.csv', low_memory=False, \n",
    "                 dtype={'MasterLC':'Int64', 'Zip Code': str})\n",
    "# pw['MasterLC'] = pw['MasterLC'].astype('Int64')\n",
    "\n",
    "# Temporarily remove all NPW.\n",
    "##pw = pw[pw['WD'] != 'N']\n",
    "\n",
    "warehouses = pw['WD'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-sessions",
   "metadata": {},
   "source": [
    "## Top-level processing and filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-diagram",
   "metadata": {},
   "source": [
    "#### Sad updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff36170-1614-4f0b-9189-ec9ba39ce5f8",
   "metadata": {},
   "source": [
    "In the Priceweight data (pw) copying CS-SKU to CS-SKU-NP - basically copying the SKU with punctuation into the non-punctuation SKU column (for Brock 'C')\\\n",
    "Setting MasterLC to 429 if MasterLC is 158 and 429\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27c6d305-4405-4193-b7bc-487dfacbfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the Line Code and CS-SKU-NP for Brock\n",
    "\n",
    "pw.loc[(pw['MasterLC']==158) & (pw['LC']=='429'), 'MasterLC'] = 429\n",
    "pw.loc[(pw['WD']=='C'), 'MasterSKU'] = pw.loc[(pw['WD']=='C'), 'MasterSKU'].str.replace('158|','429|', regex=False)\n",
    "pw.loc[(pw['WD']=='C'), 'CS-SKU'] = pw.loc[(pw['WD']=='C'), 'CS-SKU'].str.replace('158|','429|', regex=False)\n",
    "pw.loc[pw['WD']=='C', 'CS-SKU-NP'] = pw.loc[pw['WD']=='C', 'CS-SKU']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "absent-broadcasting",
   "metadata": {},
   "source": [
    "#### Preprocess price file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b5ae8-8128-4e86-9a01-6addedeccf89",
   "metadata": {},
   "source": [
    "Reading MAP (Minimum Advertized Price) from file against each SKU and dropping duplicates if any\\\n",
    "Sorting on 'CS-SKU-NP', dropping duplicates based on CS-SKU-NP and setting CS-SKU-NP as index -- This creates a unique sorted index which has performance benefits https://stackoverflow.com/questions/16626058/what-is-the-performance-impact-of-non-unique-indexes-in-pandas \\\n",
    "Basically, it can search any value in O(1) time\\\n",
    "In the end, we get a Pandas Series with SKU as index and MAP as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "endangered-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prices = pd.read_csv('maps.csv')\n",
    "map_prices = map_prices.sort_values('CS-SKU-NP').drop_duplicates(subset=['CS-SKU-NP'])\n",
    "map_prices = map_prices.set_index('CS-SKU-NP')['MAP']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c9737-5a4c-4253-a6af-8069cfab1fc6",
   "metadata": {},
   "source": [
    "Creating a new column 'item_cost' by copying 'MinPrice'\\\n",
    "Creating filter 'dorman_update_idx'=> where Warehouse is Dorman ('D') and PackQty is not na\n",
    "Using this filter to convert prices from per pack to per unit\n",
    "\n",
    "Creating backup copy of cs-sku-np with name cs-sku-np-catsku\n",
    "\n",
    "For Punctuation warehouses, Create CS-SKU-NP by concatenating Key, MasterLC, | and Part Number\\\n",
    "for non-Puncuation warehouses, remove non-alphanumeric characters from Part Number - This temporary step of copying to temporary variable 'x' can be avoided by filtering and assigning using loc function directly\n",
    "lambda operator defines a function in a single line - this function is passed as a filter to map\n",
    "\n",
    "In the Price Weight dataframe (pw), adding a new column 'MAP' (Minimum Advertized Price) with MAP prices from above step if available and 1 if not available in above data (map_prices)\\\n",
    "\n",
    "Removing WeatherTech (310) from Price Weight File\n",
    "\n",
    "Set not available values indicater (9999) back to actual nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sticky-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define separate column for managing costs vs MinPrice to avoid confusion.\n",
    "pw['item_cost'] = pw['MinPrice']\n",
    "\n",
    "# update Dorman costs (which are per-pack initally) to be per-unit\n",
    "# dorman_update_idx = (pw['WD']=='D') & (pw['PackQty'].notna())\n",
    "# pw.loc[dorman_update_idx, 'item_cost'] = pw.loc[dorman_update_idx, 'item_cost'] / pw.loc[dorman_update_idx, 'PackQty']\n",
    "# del dorman_update_idx\n",
    "pw.loc[pw['WD']=='D', 'PackQty'] = 1   #setting the PackQty for items from Dorman warehouse to 1 because they give us the price of the whole package - no need for Pack quantity multiplication\n",
    "\n",
    "# Make a \"backup\" copy of cs-sku-np for CatSKU situations\n",
    "pw['CS-SKU-NP-CatSKU'] = pw['CS-SKU-NP']\n",
    "\n",
    "#Set CSSKUNP depending on if it's a punctuation warehouse\n",
    "x = pw[pw['WD'].isin(PUNCTUATION_WAREHOUSES)].copy()\n",
    "x['CS-SKU-NP'] = x['WD'] + x['MasterLC'].astype(str) + '|' + x['Part Number']\n",
    "pw.loc[pw['WD'].isin(PUNCTUATION_WAREHOUSES), :] = x\n",
    "\n",
    "pw = pw.loc[~pw['Part Number'].isnull()] #Removing null Part Numbers\n",
    "\n",
    "x = pw[~pw['WD'].isin(PUNCTUATION_WAREHOUSES)].copy()\n",
    "x['CS-SKU-NP'] = (x['WD'] + x['MasterLC'].astype(str) + '|' \n",
    "                  + x['Part Number'].map(lambda s: ''.join(filter(str.isalnum, s))))\n",
    "pw.loc[~pw['WD'].isin(PUNCTUATION_WAREHOUSES), :] = x\n",
    "\n",
    "pw = pw.join(other=map_prices, on='CS-SKU-NP', how='left')   #optimized code for fetching MAP columnt from map_prices\n",
    "pw['MAP'].fillna(1, inplace=True)\n",
    "del map_prices   #This dataset is no longer needed\n",
    "\n",
    "# Remove WeatherTech (just in case)\n",
    "pw = pw[pw['MasterLC'] != 310]\n",
    "\n",
    "# Remove First Stop Brakes Dorman Line\n",
    "# df = df[~((df['WD']=='D') & df['Part Number'].isin(first_stop_brakes))]\n",
    "# Nope, actually don't remove them, just set MinQty really high... at the end.\n",
    "\n",
    "# Remove placeholder values for Weight/ShipWeight\n",
    "pw.loc[(pw['Weight']==9999), 'Weight'] = np.nan\n",
    "pw.loc[(pw['ShipWeight']==9999), 'ShipWeight'] = np.nan\n",
    "\n",
    "\n",
    "pw_cols = pw.columns   #columns in the Price Weight Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e4a4565-b002-4439-816d-81c8dc6f7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dorman Items pack Quantity Adjustment\n",
    "\n",
    "pw.loc[pw['MasterSKU']=='591|611034', 'PackQty'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-nirvana",
   "metadata": {},
   "source": [
    "#### Filter parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364dd923-0cd5-410e-9bc5-816c6cb80196",
   "metadata": {},
   "source": [
    "Remove Excluded warehouses from price weight data\\\n",
    "Adjusting invalid PackQty - na => 1\\\n",
    "Considering only values with PackQty <=10 or any Dorman values (we have clean data for Dorman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "progressive-prize",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Idea here is to filter out all the lil nasties that we don't want to include.\n",
    "# This could differ by warehouse, or not.\n",
    "# Things like, heavy parts, big or oddly shaped parts, \n",
    "# parts that are really expensive, or come in packs of many.\n",
    "# ... See notes on original Jim conversation for what all you should be including here.\n",
    "\n",
    "# Filter out excluded warehouses.\n",
    "pw = pw[~pw['WD'].isin(EXCLUDED_WAREHOUSES)]\n",
    "# Filter out nasty pack quantities. (allow these for Dorman, since we have clean data.)\n",
    "pw['PackQty'] = pw['PackQty'].fillna(1) # assume PackQty of NA => PackQty=1\n",
    "pw = pw[(pw['PackQty'] <= 10) | (pw['WD']=='D')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bcbf52-d5aa-48bc-85b6-a3ce7025344b",
   "metadata": {},
   "source": [
    "# Manual Inventories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238a840a-9d8c-4851-be5c-538c53ce48f7",
   "metadata": {},
   "source": [
    "#### Brock Manual Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bfe870a-45f8-4c68-b46a-bf53a8c693ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pw1 = pw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f46ad7-de57-4b4d-94b0-1f85e8e3a5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbrock = pd.read_csv('inventories/brock_20220826.csv', low_memory=False)\\nbrock['CS-SKU'] = 'C' + brock['sku_filtered_cs']\\nbrock = brock[['CS-SKU', 'prices', 'total_stock']]\\n\\n#Merging with the Brock inventory\\npw = pw.merge(brock, how='left', left_on='CS-SKU-NP', right_on='CS-SKU', suffixes=('','_b') )\\n\\npw.loc[~pw['prices'].isnull(), 'MinPrice'] = pw.loc[~pw['prices'].isnull(), 'prices']\\npw.loc[~pw['prices'].isnull(), 'item_cost'] = pw.loc[~pw['prices'].isnull(), 'prices']\\n\\n#updating stock from the brock inventory\\npw.loc[~pw['prices'].isnull(), 'Qty'] = pw.loc[~pw['prices'].isnull(), 'total_stock']\\n\\npw.loc[((pw['prices'].isnull()) & (pw['WD']=='C')), 'Qty'] = 0  #Setting Quantity of items not present in inventory file to zero\\n\\n\\n#Restoring pw columns from before brock manual update\\npw = pw[pw_cols]\\n\\ndel brock #Delete Brock Inventory Dataframe as it is no longer needed\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "brock = pd.read_csv('inventories/brock_20220826.csv', low_memory=False)\n",
    "brock['CS-SKU'] = 'C' + brock['sku_filtered_cs']\n",
    "brock = brock[['CS-SKU', 'prices', 'total_stock']]\n",
    "\n",
    "#Merging with the Brock inventory\n",
    "pw = pw.merge(brock, how='left', left_on='CS-SKU-NP', right_on='CS-SKU', suffixes=('','_b') )\n",
    "\n",
    "pw.loc[~pw['prices'].isnull(), 'MinPrice'] = pw.loc[~pw['prices'].isnull(), 'prices']\n",
    "pw.loc[~pw['prices'].isnull(), 'item_cost'] = pw.loc[~pw['prices'].isnull(), 'prices']\n",
    "\n",
    "#updating stock from the brock inventory\n",
    "pw.loc[~pw['prices'].isnull(), 'Qty'] = pw.loc[~pw['prices'].isnull(), 'total_stock']\n",
    "\n",
    "pw.loc[((pw['prices'].isnull()) & (pw['WD']=='C')), 'Qty'] = 0  #Setting Quantity of items not present in inventory file to zero\n",
    "\n",
    "\n",
    "#Restoring pw columns from before brock manual update\n",
    "pw = pw[pw_cols]\n",
    "\n",
    "del brock #Delete Brock Inventory Dataframe as it is no longer needed\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97162be6-8f8c-4bae-92f3-749083f8826d",
   "metadata": {},
   "source": [
    "#### NPW Manual Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "060f45e0-e0cc-4844-a8fa-1c2ef9b1fdbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnpw = pd.read_csv('inventories/npw_20220826.csv', low_memory=False)\\nnpw['Core'].fillna(0, inplace=True)\\nnpw['WD'] = 'N'\\nnpw.rename(columns={'PartN':'Part Number', 'LineCode':'LC'}, inplace=True )\\nnpw['Cost'] += npw['Core']   #Add core amount to Cost\\nnpw.drop(columns = ['Core'], inplace=True) #Drop the Core column as it has already been added to the cost\\nnpw['Part Number'] = npw['Part Number'].map(lambda s: ''.join(filter(str.isalnum, s)))   #Removing Punctuation from NPW Part Numbers\\n\\npw = pw.merge(npw, how='left', on = ['WD', 'LC', 'Part Number'], suffixes=('', '_npw') )   #Merging with NPW inventory\\npw.loc[~pw['QA'].isnull(), 'Qty'] = pw.loc[~pw['QA'].isnull(), 'QA']   #Updating Quantity directly from NPW inventory\\npw.loc[~pw['QA'].isnull(), 'MinPrice'] = pw.loc[~pw['QA'].isnull(), 'Cost']   #Updating MinPrice and Cost directly from NPW Inventory\\npw.loc[~pw['QA'].isnull(), 'item_cost'] = pw.loc[~pw['QA'].isnull(), 'Cost']\\n\\npw.loc[((pw['QA'].isnull()) & (pw['WD']=='N')), 'Qty'] = 0   #Setting Quantity for NPW items not found in NPW inventory to 0\\n\\npw = pw[pw_cols]   #Restoring pw columns from before npw manual update\\ndel npw   #Delete npw Inventory Dataframe as it is no longer needed\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "npw = pd.read_csv('inventories/npw_20220826.csv', low_memory=False)\n",
    "npw['Core'].fillna(0, inplace=True)\n",
    "npw['WD'] = 'N'\n",
    "npw.rename(columns={'PartN':'Part Number', 'LineCode':'LC'}, inplace=True )\n",
    "npw['Cost'] += npw['Core']   #Add core amount to Cost\n",
    "npw.drop(columns = ['Core'], inplace=True) #Drop the Core column as it has already been added to the cost\n",
    "npw['Part Number'] = npw['Part Number'].map(lambda s: ''.join(filter(str.isalnum, s)))   #Removing Punctuation from NPW Part Numbers\n",
    "\n",
    "pw = pw.merge(npw, how='left', on = ['WD', 'LC', 'Part Number'], suffixes=('', '_npw') )   #Merging with NPW inventory\n",
    "pw.loc[~pw['QA'].isnull(), 'Qty'] = pw.loc[~pw['QA'].isnull(), 'QA']   #Updating Quantity directly from NPW inventory\n",
    "pw.loc[~pw['QA'].isnull(), 'MinPrice'] = pw.loc[~pw['QA'].isnull(), 'Cost']   #Updating MinPrice and Cost directly from NPW Inventory\n",
    "pw.loc[~pw['QA'].isnull(), 'item_cost'] = pw.loc[~pw['QA'].isnull(), 'Cost']\n",
    "\n",
    "pw.loc[((pw['QA'].isnull()) & (pw['WD']=='N')), 'Qty'] = 0   #Setting Quantity for NPW items not found in NPW inventory to 0\n",
    "\n",
    "pw = pw[pw_cols]   #Restoring pw columns from before npw manual update\n",
    "del npw   #Delete npw Inventory Dataframe as it is no longer needed\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699cc19e-bf33-4386-8158-689bac39606e",
   "metadata": {},
   "source": [
    "#### PFG Manual Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6670683-d6da-4a16-a8e6-ee78dab37434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\npfg = pd.read_csv('inventory/pfg.txt', sep='\\t', encoding_errors='ignore', escapechar='\\\\', low_memory=False, usecols=['SKU', 'STOCK_TOTAL', 'COST'] )\\npfg['WD'] = 'J'\\npfg.set_index( keys=['WD', 'SKU'], inplace=True)\\n\\npw = pw.join(other=pfg, on=['WD', 'Part Number'], how='left')   #Joining the price weight report with PFG inventory\\n\\npfg_inv_fil = (pw['WD']=='J') & (~(pw['STOCK_TOTAL'].isnull()))   #creating filter for items found in PFG inventory\\n\\n#Updating Stock and Cost from PFG inventory into the price weight report\\npw.loc[pfg_inv_fil, 'MinPrice'] =pw.loc[pfg_inv_fil, 'COST']\\npw.loc[pfg_inv_fil, 'item_cost'] =pw.loc[pfg_inv_fil, 'COST']\\npw.loc[pfg_inv_fil, 'Qty'] =pw.loc[pfg_inv_fil, 'STOCK_TOTAL']\\n\\npw.loc[(pw['WD']=='J') & (pw['STOCK_TOTAL'].isnull()), 'Qty'] = 0   #Setting the stock of PFG items not in PFG inventory to 0\\npw.drop(columns=['STOCK_TOTAL', 'COST'], inplace=True)\\n\\ndel pfg_inv_fil\\ndel pfg\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "pfg = pd.read_csv('inventory/pfg.txt', sep='\\t', encoding_errors='ignore', escapechar='\\\\', low_memory=False, usecols=['SKU', 'STOCK_TOTAL', 'COST'] )\n",
    "pfg['WD'] = 'J'\n",
    "pfg.set_index( keys=['WD', 'SKU'], inplace=True)\n",
    "\n",
    "pw = pw.join(other=pfg, on=['WD', 'Part Number'], how='left')   #Joining the price weight report with PFG inventory\n",
    "\n",
    "pfg_inv_fil = (pw['WD']=='J') & (~(pw['STOCK_TOTAL'].isnull()))   #creating filter for items found in PFG inventory\n",
    "\n",
    "#Updating Stock and Cost from PFG inventory into the price weight report\n",
    "pw.loc[pfg_inv_fil, 'MinPrice'] =pw.loc[pfg_inv_fil, 'COST']\n",
    "pw.loc[pfg_inv_fil, 'item_cost'] =pw.loc[pfg_inv_fil, 'COST']\n",
    "pw.loc[pfg_inv_fil, 'Qty'] =pw.loc[pfg_inv_fil, 'STOCK_TOTAL']\n",
    "\n",
    "pw.loc[(pw['WD']=='J') & (pw['STOCK_TOTAL'].isnull()), 'Qty'] = 0   #Setting the stock of PFG items not in PFG inventory to 0\n",
    "pw.drop(columns=['STOCK_TOTAL', 'COST'], inplace=True)\n",
    "\n",
    "del pfg_inv_fil\n",
    "del pfg\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa4d1eb-92bb-403b-bd70-b0424e06808d",
   "metadata": {},
   "source": [
    "#### Dorman Manual Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33147955-5b7d-48af-beac-aef8fc850f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndprice = pd.read_csv('inventory/dorman_price.csv', low_memory=False)\\ndprice['WD'] = 'D'\\ndprice['COST'] = dprice['Cost'] + dprice['Core']\\ndprice.drop(columns=['Cost', 'Core'], inplace=True)\\ndprice['PN'] = dprice['PN'].map(lambda s: ''.join(filter(str.isalnum, s)))\\ndprice.set_index(['WD', 'LC', 'PN'], inplace=True)\\n\\npw = pw.join(dprice, on=['WD', 'LC', 'Part Number'], how='left')   #Joining pw-all with Dorman Price File\\n\\npw.loc[~pw['COST'].isnull(), 'MinPrice'] = pw.loc[~pw['COST'].isnull(), 'COST']  #Updating Prices from Dorman Price File\\npw.loc[~pw['COST'].isnull(), 'item_cost'] = pw.loc[~pw['COST'].isnull(), 'COST']\\npw.drop(columns='COST', inplace=True)\\ndel dprice\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dprice = pd.read_csv('inventory/dorman_price.csv', low_memory=False)\n",
    "dprice['WD'] = 'D'\n",
    "dprice['COST'] = dprice['Cost'] + dprice['Core']\n",
    "dprice.drop(columns=['Cost', 'Core'], inplace=True)\n",
    "dprice['PN'] = dprice['PN'].map(lambda s: ''.join(filter(str.isalnum, s)))\n",
    "dprice.set_index(['WD', 'LC', 'PN'], inplace=True)\n",
    "\n",
    "pw = pw.join(dprice, on=['WD', 'LC', 'Part Number'], how='left')   #Joining pw-all with Dorman Price File\n",
    "\n",
    "pw.loc[~pw['COST'].isnull(), 'MinPrice'] = pw.loc[~pw['COST'].isnull(), 'COST']  #Updating Prices from Dorman Price File\n",
    "pw.loc[~pw['COST'].isnull(), 'item_cost'] = pw.loc[~pw['COST'].isnull(), 'COST']\n",
    "pw.drop(columns='COST', inplace=True)\n",
    "del dprice\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc42d8-25eb-4ebb-b4d0-c835b963b873",
   "metadata": {},
   "source": [
    "Manual Update verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b1ae879-0d0d-42db-83f7-b9bb6df5eff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntmp = pw.merge (pw1, on=[\\'WD\\', \\'LC\\', \\'Part Number\\'], suffixes = (\\'_n\\', \\'_p\\' ) )\\nprint (tmp.query(\"MinPrice_n != MinPrice_p\").groupby(\\'WD\\').size())\\nprint (tmp.query(\"Qty_n != Qty_p\").groupby(\\'WD\\').size())\\ndel tmp\\ndel pw1\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "tmp = pw.merge (pw1, on=['WD', 'LC', 'Part Number'], suffixes = ('_n', '_p' ) )\n",
    "print (tmp.query(\"MinPrice_n != MinPrice_p\").groupby('WD').size())\n",
    "print (tmp.query(\"Qty_n != Qty_p\").groupby('WD').size())\n",
    "del tmp\n",
    "del pw1\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-palestine",
   "metadata": {},
   "source": [
    "#### Calculate shipping by warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735e7348-303d-4ca8-be38-a0fbb3b453ae",
   "metadata": {},
   "source": [
    "Read Shipping Data from file\\\n",
    "create column 'warehouse_key' in this dataframe by fetching key using the 'get_warehouse_key' function\n",
    "Filter to include entries with Quantity >0 - exclude Quantities=0\\\n",
    "Converting 'Ship Cost' to per item\n",
    "Filtering records with Shipping Cost > 0.05 and only considering a few columns\n",
    "\n",
    "Reading price-file-shipping.csv and converting it all to a single dictionary | Using a series with unique index should give us the same performance\n",
    "\n",
    "#### get_historical_shipping_estimates(df, warehouse_key):\n",
    "    Filter the main (total) shipping data to the provided warehouse, join/merge df to this filtered shipping data based on SKU creating 'ship_weights'. Filter the records with weights between 0 and 1000 (removing garbage).\n",
    "    If some clean data is available after this filtering, fit a LinearRegression() model on it to prodict Ship Cost based on dimensions and weights of the items. Displaying the RMS error of this trained model - Train, Test split missing here\n",
    "    \n",
    "##### Shipping cost prediction:\n",
    "    if the sku is available in the recent price-file-shipping, use that shipping cost,\n",
    "    if their is an order history of the sku, use the mean of the shipping cost of all historical orders\n",
    "    if the shipping cost (LinearRegression) model exists for the warehouse (i.e. clean data is avaialable), predict the shipping cost using this model\n",
    "    if their is some historical data available for that warehouse, use its mean\n",
    "    else use 10 as shipping cost\n",
    "    \n",
    "#### calculate_warehouse_shipping(df, warehouse):\n",
    "Expects a price/ weight dataframe format filtered for the warehouse. Returns with shipping price altered.\n",
    "Assigns shipping values for each warehouse as per its rules.\\\n",
    "##### pfg\n",
    "Checks the pfg inventory file to check for shipping and handling costs of the items. Sums these two costs and adds $3.5 to account for taxes. There should be some other way then this loop. (A simple join would probably be better for performance). If the item isn't found in the inventory file, use PFG_DEFAULT_SHIPPING (15)\n",
    "\n",
    "##### Keystone\n",
    "KEYSTONE_BASE_SHIPPING=11, KEYSTONE_LTL_SHIPPING=125\n",
    "\n",
    "##### NPW\n",
    "Possible overwrite of AC Delco SKUs (ignoring the 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb2e33cd-cf51-4084-98dc-d8a4f2573d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shipping_data = pd.read_csv('shipping_data.csv', low_memory=False, parse_dates=['Ship Date'])\n",
    "shipping_data['warehouse_key'] = shipping_data['Warehouse'].map(get_warehouse_key)\n",
    "shipping_data = shipping_data[shipping_data['Quantity'] > 0]\n",
    "shipping_data['Ship Cost'] = shipping_data['Ship Cost'] / shipping_data['Quantity']\n",
    "shipping_data = shipping_data[shipping_data['Ship Cost'] > 0.05][['CS-SKU','warehouse_key','Ship Cost','Ship Date']]\n",
    "\n",
    "price_file_shipping = pd.read_csv('price-file-shipping.csv', index_col='CS-SKU-NP')   #Optimized code instead of loop for dictionary conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1217e84e-cbc6-42be-9f51-928bf6dfa68e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_historical_shipping_estimates(df, warehouse_key): # (cssku, warehouse_key):    \n",
    "    warehouse_shipping_data = shipping_data[(shipping_data['warehouse_key']==warehouse_key)].copy()\n",
    "\n",
    "    # create warehouse-level shipping model\n",
    "    feature_cols = ['Weight', 'DimWeight', 'ShipWeight', 'Length', 'Width', 'Height']\n",
    "    ship_weights = df[['MasterSKU']+feature_cols].merge(warehouse_shipping_data, how='inner',\n",
    "                                                        left_on='MasterSKU', right_on='CS-SKU').copy().dropna()\n",
    "    ship_weights = ship_weights[(ship_weights['ShipWeight'] > 0) & (ship_weights['ShipWeight'] < 1000)]  \n",
    "    if len(ship_weights) > 0:\n",
    "        model = LinearRegression().fit(ship_weights[feature_cols], ship_weights['Ship Cost'])\n",
    "\n",
    "         # log model error for audit purposes\n",
    "        print('RMSE:',mean_squared_error(ship_weights['Ship Cost'], \n",
    "                                          model.predict(ship_weights[feature_cols]), squared=False))\n",
    "    else:\n",
    "         model = None\n",
    "\n",
    "\n",
    "    #Getting Shipping price from recent price file\n",
    "    df['cssku'] = df['WD']+df['MasterSKU']   #Creating cssku to fetch data from recent price file\n",
    "    df = df.join(other=price_file_shipping, on='cssku', how='left', rsuffix='_pfs')\n",
    "    df['Shipping'] = df['Shipping_pfs']\n",
    "    df.drop(columns=['cssku', 'Shipping_pfs'], inplace=True)\n",
    "\n",
    "    #Getting Mean of Shipping from Historical Shipping Data\n",
    "    h_ship_avg = warehouse_shipping_data.groupby('CS-SKU').mean()['Ship Cost']\n",
    "    df = df.join(other=h_ship_avg, how='left', on='MasterSKU')\n",
    "    df.loc[df['Shipping'].isna(), 'Shipping'] = df.loc[df['Shipping'].isna(), 'Ship Cost']\n",
    "    df.drop(columns=['Ship Cost'], inplace=True)\n",
    "\n",
    "    #Getting Shipping Prediction from Dimensional LinearRegression model\n",
    "    if len(ship_weights) > 0:\n",
    "        df['ship_model'] = model.predict(df[feature_cols].fillna(0))\n",
    "        df.loc[df['Shipping'].isna(), 'Shipping'] = df.loc[df['Shipping'].isna(), 'ship_model']\n",
    "        df.drop(columns=['ship_model'], inplace=True)\n",
    "\n",
    "    \n",
    "    #Getting Warehouse Shipping Mean\n",
    "    h_warehouse_avg = warehouse_shipping_data.groupby('warehouse_key').mean()['Ship Cost']\n",
    "    df = df.join(other=h_warehouse_avg, on='WD')\n",
    "    df.loc[df['Shipping'].isna(), 'Shipping'] = df.loc[df['Shipping'].isna(), 'Ship Cost']\n",
    "    df.drop(columns=['Ship Cost'], inplace=True)\n",
    "\n",
    "    #Baseline value of 10\n",
    "    df.loc[df['Shipping'].isna(), 'Shipping'] = 10\n",
    "    \n",
    "    return df['Shipping'].values\n",
    "\n",
    "# Expects something formatted like a price/weight DF, filtered for a warehouse\n",
    "# returns the price/weight DF with shipping altered\n",
    "def calculate_warehouse_shipping(df, warehouse):\n",
    "\n",
    "    df = df.copy()\n",
    "    print(warehouse)\n",
    "    if warehouse=='D': # Dorman        \n",
    "        #df.loc[(df['item_cost'] <= 30), ['Shipping', 'ShipMkup']] = 6, DEFAULT_SHIP_MARKUP # flat rate\n",
    "        #df.loc[(df['item_cost'] > 30), ['Shipping', 'ShipMkup']] = 0, 1\n",
    "        df.loc[:, ['Shipping', 'ShipMkup']] = 0, 1\n",
    "        #df['ShipMkup'] = 1\n",
    "        \n",
    "    elif warehouse=='C': # Brock\n",
    "        df.loc[(df['item_cost'] <= 50), 'Shipping'] = 12 # estimate / avg.\n",
    "        df.loc[(df['item_cost'] > 50), 'Shipping'] = 0\n",
    "        df['ShipMkup'] = 1 / 1.1 # to account for returns not being accepted\n",
    "    elif warehouse=='P': # Parts Auth\n",
    "        df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "        '''\n",
    "        with open(PARTS_AUTH_SHIPPING_MODEL, 'rb') as f:\n",
    "            m = pickle.load(f)\n",
    "        df['lwh'] = df['Length'] * df['Width'] * df['Height']\n",
    "        df['Shipping'] = m.predict(df[['Weight','Length','Width','Height','lwh']].fillna(0))\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "        '''\n",
    "    elif warehouse=='1': # Jante\n",
    "        df['Shipping'] = 0\n",
    "        df['ShipMkup'] = 1\n",
    "    elif warehouse=='J': # PFG        \n",
    "        #Reading Shipping and Handling Cost from PGF inventory file and using their sum as Shipping Cost\n",
    "        PFG_DEFAULT_SHIPPING = 15\n",
    "        inv = pd.read_csv('inventory/pfg.txt', sep='\\t', encoding_errors='ignore', escapechar='\\\\', low_memory=False, usecols=['SKU','SHIPPING_COST','HANDLING_COST'], index_col='SKU' )\n",
    "        inv['pfg_cost'] = inv[['SHIPPING_COST', 'HANDLING_COST']].sum(1)\n",
    "        inv.drop(columns=['SHIPPING_COST','HANDLING_COST'], inplace=True)\n",
    "        df = df.join(other=inv, on='Part Number', how='left')\n",
    "        df.loc[df['WD']=='J', 'pfg_cost'].fillna(PFG_DEFAULT_SHIPPING, inplace=True)\n",
    "        df.loc[df['WD']=='J', 'Shipping'] = df.loc[df['WD']=='J', 'pfg_cost']\n",
    "        df.drop(columns='pfg_cost', inplace=True)\n",
    "\n",
    "        # ADD AN EXTRA $3.50 TO ACCOUNT FOR TAX MESS\n",
    "        df['Shipping'] = df['Shipping'] + 3.50 \n",
    "        \n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    elif warehouse=='K': # Keystone\n",
    "        KEYSTONE_BASE_SHIPPING = 11.0\n",
    "        KEYSTONE_LTL_SHIPPING = 175.0\n",
    "        inv = pd.read_csv('inventory/keystone.csv', low_memory=False)\n",
    "        # Since inventory file open, manage duplicate part # issue in Keystone by matching with UPC\n",
    "        inv['PartNumber'] = inv['PartNumber'].str.replace('=','').str.replace('\"','')\n",
    "        ##inv['KeystoneShipping'] = (inv['UPS_Ground_Assessorial'] + KEYSTONE_BASE_SHIPPING).fillna(0)\n",
    "        inv['KeystoneShipping'] = (KEYSTONE_BASE_SHIPPING)\n",
    "        inv.loc[inv['UPSable']==False, 'KeystoneShipping'] = KEYSTONE_LTL_SHIPPING\n",
    "        inv = inv.sort_values('KeystoneShipping', ascending=False).drop_duplicates(subset=['VendorCode','PartNumber'])\n",
    "        df = df.merge(inv[['VendorCode','PartNumber','KeystoneShipping']], \n",
    "                      how='left', left_on=['LC','Part Number'], right_on=['VendorCode','PartNumber'])\n",
    "        df['Shipping'] = df['KeystoneShipping']\n",
    "        df['ShipMkup'] = 1\n",
    "\n",
    "    elif warehouse=='6': # Burco Mirrors\n",
    "        df['Shipping'] = 8 # estimate / avg        \n",
    "        df['ShipMkup'] = 1\n",
    "    elif warehouse=='A': # APW\n",
    "        df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    elif warehouse=='2': # OE Wheels\n",
    "        df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    elif warehouse=='5': # KSI Trading\n",
    "        df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    \n",
    "    elif warehouse=='7': # NTW\n",
    "        df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    elif warehouse=='8': # Race Sport Lighting\n",
    "        ##df['Shipping'] = df['Weight'].map(lambda w: 15 if (pd.isna(w) or w >= 1) else 6)\n",
    "        ##df['ShipMkup'] = DEFAULT_SHIP_MARKUP        \n",
    "        df['Shipping'] = 23.5\n",
    "        df['ShipMkup'] = 1        \n",
    "    elif warehouse=='9': # Sunbelt APG\n",
    "        df['Shipping'] = 22\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    \n",
    "    elif warehouse=='N': # NPW\n",
    "        df['Shipping'] = 6\n",
    "        df.loc[df['MinPrice']>15, 'Shipping'] = 10\n",
    "        df.loc[df['MinPrice']>30, 'Shipping'] = 15\n",
    "        df.loc[df['MinPrice']>50, 'Shipping'] = 20\n",
    "        df.loc[df['MinPrice']>80, 'Shipping'] = 35\n",
    "        df.loc[df['MinPrice']>150, 'Shipping'] = 60\n",
    "        df.loc[df['MinPrice']>300, 'Shipping'] = 100\n",
    "        df.loc[df['MinPrice']>500, 'Shipping'] = 200\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "    \n",
    "    elif warehouse=='O': # Tonsa\n",
    "        df['Shipping'] = get_historical_shipping_estimates(df, warehouse)\n",
    "        df['ShipMkup'] = DEFAULT_SHIP_MARKUP\n",
    "        \n",
    "    elif warehouse=='Y': # MotorState\n",
    "        df.loc[(df['item_cost'] <= 39.99), 'Shipping'] = 13\n",
    "        df.loc[((df['item_cost'] > 39.99) & (df['item_cost'] <= 99.99)), 'Shipping'] = 12\n",
    "        df.loc[(df['item_cost'] >= 100), 'Shipping'] = 11\n",
    "        df['ShipMkup'] = 1        \n",
    "    \n",
    "    elif warehouse=='Z': # SimpleTire\n",
    "        df['Shipping'] = 0        \n",
    "        df['ShipMkup'] = 1     \n",
    "    \n",
    "    elif warehouse=='F':   #Keystone Crash (LKQ)\n",
    "        LKQ = pd.read_csv('LKQ Shipping.csv', low_memory=False, usecols=['Product', 'Ship Cost'])   #Reading LKQ Ship Cost file from disk\n",
    "        LKQ['WD'] = 'F'   \n",
    "        df = df.merge(LKQ, how='left', left_on=['WD', 'Part Number'], right_on=['WD', 'Product'])   #Merging Price Weight Report with LKQ Ship Cost file\n",
    "        df.loc[df['WD'] == 'F', 'Shipping'] = df.loc[df['WD'] == 'F', 'Ship Cost']\n",
    "        df.drop (columns=['Product', 'Ship Cost'], inplace=True)\n",
    "        del LKQ   #Deleting the LKQ Ship cost file from memory as it is no longer needed\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    df['ShipMkup'] = df['ShipMkup'].fillna(1)\n",
    "    return df[['Shipping','ShipMkup']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac403e15-8384-49cf-bb3f-d96984a729f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67faa46d-1b6b-4509-9aeb-1746dde0239c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "Proportion of parts missing shipping: 0.0\n",
      "D\n",
      "Proportion of parts missing shipping: 0.0\n",
      "1\n",
      "Proportion of parts missing shipping: 0.0\n",
      "2\n",
      "Proportion of parts missing shipping: 0.0\n",
      "4\n",
      "Proportion of parts missing shipping: 1.0\n",
      "6\n",
      "Proportion of parts missing shipping: 0.0\n",
      "8\n",
      "Proportion of parts missing shipping: 0.0\n",
      "9\n",
      "Proportion of parts missing shipping: 0.0\n",
      "K\n",
      "Proportion of parts missing shipping: 0.15514241230881948\n",
      "F\n",
      "Proportion of parts missing shipping: 0.02563554224919137\n",
      "Y\n",
      "Proportion of parts missing shipping: 0.0\n",
      "N\n",
      "Proportion of parts missing shipping: 0.0\n",
      "P\n",
      "RMSE: 5.584798856952385\n",
      "Proportion of parts missing shipping: 0.0\n",
      "J\n",
      "Proportion of parts missing shipping: 0.07701080173860653\n",
      "Z\n",
      "Proportion of parts missing shipping: 0.0\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for warehouse in pw['WD'].unique().tolist():\n",
    "    wdf = pw[pw['WD']==warehouse].copy()\n",
    "    wdf.loc[:, ['Shipping','ShipMkup']] = calculate_warehouse_shipping(wdf.loc[wdf['WD']==warehouse, :], \n",
    "                                                                           warehouse).values\n",
    "    print('Proportion of parts missing shipping:', wdf['Shipping'].isna().mean())\n",
    "    dfs.append(wdf)\n",
    "pw = pd.concat(dfs, ignore_index=True)\n",
    "pw['ShipMkup'] = pw['ShipMkup'].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0835d3ed-4bb5-4927-8124-6fdcc6147712",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting datasets from memory which are no longer needed\n",
    "del shipping_data, price_file_shipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-female",
   "metadata": {},
   "source": [
    "#### Set inventory constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "intended-capability",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pw.loc[:, ['MinQty','MaxQty']] = 3, 12   ## Changed Min to two from three\n",
    "\n",
    "pw.loc[pw['MasterLC']==145, ['MinQty','MaxQty']] = 10, 24   ## Updating min and Max Qty for Simple tire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-mumbai",
   "metadata": {},
   "source": [
    "#### Set price file defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "arbitrary-southwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw['ListMkup'] = .65\n",
    "pw['SourceQty'] = None\n",
    "pw['Source'] = None\n",
    "pw['BundleSK'] = None\n",
    "pw['Carrier'] = 'FedEx'\n",
    "pw['Service'] = 'GroundHD'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19661ab1-69de-4d12-9aae-44c470a98614",
   "metadata": {},
   "source": [
    "Set OP-Lowest(Y) and VND-Lowest(Y) to \"N\" for Dorman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8328495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[(pw['CS-SKU-NP'].str[0]=='D'), ['OP-Lowest(Y)','VND-Lowest(Y)'] ] = \"N\", \"N\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollywood-coordinate",
   "metadata": {},
   "source": [
    "## Quick fix for shipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-staff",
   "metadata": {},
   "source": [
    "Where shipping is current zero for Tonsa, set it to 20. And 15 for Sunbelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "skilled-laser",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[((pw['Shipping'] == 0) & (pw['CS-SKU-NP'].str[0] == 'O')), 'Shipping'] = 20\n",
    "pw.loc[((pw['Shipping'] == 0) & (pw['CS-SKU-NP'].str[0] == '9')), 'Shipping'] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-mongolia",
   "metadata": {},
   "source": [
    "1.5x ~Tonsa, Sunbelt~, and Parts Auth shipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fifty-drove",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pw.loc[(pw['CS-SKU-NP'].str[0].isin(['O','9','P'])), 'Shipping'] *= 1.5\n",
    "pw.loc[(pw['CS-SKU-NP'].str[0]=='P'), 'Shipping'] *= 1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-aluminum",
   "metadata": {},
   "source": [
    "Double Eagle Eye Shipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "alleged-grocery",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[(pw['CS-SKU-NP'].str[:4]=='P754'), 'Shipping'] *= 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "circular-exemption",
   "metadata": {},
   "source": [
    "Bumper is expensive to ship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "treated-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='429|6448-0006', 'Shipping'] = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-government",
   "metadata": {},
   "source": [
    "Another expensive shipping update from order: PSA669874628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "proud-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='551|S6585B', 'Shipping'] = 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834e8265",
   "metadata": {},
   "source": [
    "Expensive shipping for part from 12/20/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c24bfec-0351-4002-b347-1c2adf6dbe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P576|3292', 'Shipping'] = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b37bf81",
   "metadata": {},
   "source": [
    "Expensive shipping for part from 12/27/2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "022907cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P550|290073', 'Shipping'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d6fb22b-e3c0-4093-ad08-dd6d2f1212e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P308|55621', 'Shipping'] = 161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6391f25-f8ac-4346-a902-6579aa63bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P557|277504', 'Shipping'] = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f247128-2913-439f-9824-37112a1dbf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P643|ESK5752', 'Shipping'] = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8271b50-33b8-4f5b-94e6-512cd7e64d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P551|40722A', 'Shipping'] = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f804e63b-1bc7-48e8-819b-87c2488f2dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='N643|AR8265XPR', 'PackQty'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8bc3580-3b4f-4d21-905e-119dd8993309",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='P123|33660', 'PackQty'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f2488-c540-4666-844d-75f230184d68",
   "metadata": {},
   "source": [
    "PA Shipping Costs from Umer analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1bd9a1b1-5a7e-4fa2-9b00-95a22b693168",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_shipping_data = pd.read_csv('PA Shipping Costs.csv', low_memory=False)\n",
    "pa_shipping_data['WD'] = 'P'\n",
    "\n",
    "pa_shipping_data = pa_shipping_data.sort_values(by='Row Labels', ascending=False)\n",
    "pa_shipping_data.drop_duplicates(subset='Row Labels', keep=\"first\")\n",
    "\n",
    "pw = pw.merge(pa_shipping_data, how='left', left_on=['MasterSKU', 'WD'], right_on=['Row Labels', 'WD'])\n",
    "pw.loc[pw['Final Shipping Cost'] > 0, 'Shipping'] = pw['Final Shipping Cost']\n",
    "pw = pw[pw_cols]   #Restoring columns before this Merge\n",
    "\n",
    "del pa_shipping_data   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22038c91-f4c2-4b77-ab00-0825628631f4",
   "metadata": {},
   "source": [
    "NPW Pack Corrections and updates from Abdullah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "206c1bd7-919b-43b9-8575-d292c5264461",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[pw['CS-SKU-NP']=='N223|97469', 'PackQty'] = 10\n",
    "pw.loc[pw['CS-SKU-NP']=='N178|VL1093', 'PackQty'] = 4\n",
    "pw.loc[pw['CS-SKU-NP']=='N223|98288', 'PackQty'] = 10 #PackQty issue - 20-May-2022\n",
    "pw.loc[pw['CS-SKU-NP']== 'N114|3025', 'PackQty'] = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "062e9bc2-54ca-441d-a2d1-8ba108169757",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw['MasterSKUn'] = pw['CS-SKU-NP'].str[1:]\n",
    "shipping_fix = pd.read_csv('Shipping Corrections.csv', low_memory=False, encoding='unicode_escape')\n",
    "shipping_fix = shipping_fix.drop_duplicates(['WD', 'SKU'])\n",
    "\n",
    "pw = pw.merge(shipping_fix, how='left', left_on=['MasterSKUn', 'WD'], right_on=['SKU', 'WD'])\n",
    "pw.loc[pw['Final Shipping Cost'] > 0, 'Shipping'] = pw['Final Shipping Cost']\n",
    "pw = pw[pw_cols]\n",
    "\n",
    "del shipping_fix   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2145f776-fdaa-4320-8388-fc6a5d532e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shipping = pd.read_csv('newshipcosts.csv', low_memory=False)\n",
    "\n",
    "pw = pw.merge(new_shipping, how='left', left_on=['MasterSKU'], right_on=['CS_SKU'])\n",
    "pw.loc[pw['NewShipCost'] > 0, 'Shipping'] = pw['NewShipCost']\n",
    "pw = pw[pw_cols]\n",
    "\n",
    "del new_shipping   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advisory-convenience",
   "metadata": {},
   "source": [
    "Set MinQty really high for First Stop Brakes (Dorman line) to avoid actually selling any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "sunrise-detection",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_stop_brakes = pd.read_excel('8-2 Change 56 Brake Dropship and Stocking.xlsx', \n",
    "                                  skiprows=2, sheet_name='Dropship Price').rename(columns={'MATERIAL':'pn'})['pn']\n",
    "pw.loc[(pw['WD']=='D') & pw['Part Number'].isin(first_stop_brakes), ['MinQty','MaxQty']] = 100, 100\n",
    "\n",
    "del first_stop_brakes   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347dcb1",
   "metadata": {},
   "source": [
    "Handle RSL skus for MAP > Calculated Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2751520c-6d4f-48c5-8d04-3634d3727be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading RSL Inventory and Removing Duplicate SKUs\n",
    "rsl_inventory = pd.read_csv('inventory/rsl.csv', low_memory=False)\n",
    "rsl_inventory.sort_values(by='MAP', ascending=False, inplace=True)\n",
    "rsl_inventory.drop_duplicates(subset=['SKU'], inplace=True, keep='first')\n",
    "rsl_inventory['CS-SKU-NP'] = '8329|' + rsl_inventory['SKU']\n",
    "rsl_inventory.set_index('CS-SKU-NP', inplace=True)\n",
    "\n",
    "\n",
    "pw = pw.join(other = rsl_inventory, on='CS-SKU-NP', rsuffix='_rsl')\n",
    "pw['tmp_mkup'] = pw['item_cost'] / ( (1 + 0.05) * (pw['item_cost'] + pw['Shipping']) / (1 - 0.15) - pw['Shipping'])\n",
    "pw['tmp_price'] = pw['item_cost']/pw['tmp_mkup'] + pw['Shipping']/pw['ShipMkup']\n",
    "\n",
    "pw.loc[pw['tmp_price']<pw['MAP_rsl'], ['MinPrice', 'item_cost']] = pw.loc[pw['tmp_price']<pw['MAP_rsl'], 'MAP_rsl']\n",
    "pw.loc[pw['tmp_price']<pw['MAP_rsl'], ['Shipping', 'ShipMkup']] = 0\n",
    "\n",
    "pw = pw[pw_cols]   #Restoring columns\n",
    "del rsl_inventory   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "063bb6f9-dcd9-4059-bc3d-f33be1383962",
   "metadata": {},
   "outputs": [],
   "source": [
    "motorstate = pd.read_csv('inventory/motorstate.csv', low_memory=False)\n",
    "#motorstate['PartNumber'] = motorstate.PartNumber.str[3:]\n",
    "motorstate['PartNumber'] = 'Y' + motorstate['PartNumber']\n",
    "#pw = pw.merge(motorstate, how='left', left_on=['Part Number', 'WD'], right_on=['PartNumber', 'WD'])\n",
    "\n",
    "pw['tkey'] = pw['WD'] + pw['LC'] + pw['Part Number']\n",
    "\n",
    "pw = pw.merge(motorstate, how='left', left_on=['tkey'], right_on=['PartNumber'])\n",
    "pw.drop(columns='tkey', inplace = True)\n",
    "\n",
    "del motorstate   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c06c807d-b4e6-41a7-b23f-111ed7dd638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.drop(pw.loc[pw['AirRestricted'] == 'YES'].index, inplace=True)\n",
    "pw.drop(pw.loc[pw['TruckFrtOnly'] == 'YES'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da908c40-bb20-4fd4-b877-07f4d97790d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pw.drop(pw.loc[(pw['WD'] == 'Y') & (pw['MasterLC'] != 261)].index, inplace=True)\n",
    "pw = pw [pw_cols]   #Restoring Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dec6179-f238-4d07-a1b5-11d7fc313a93",
   "metadata": {},
   "source": [
    "NPW Min Order Qty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f5d860b-7ff6-4b15-af98-ecaea9f636ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnpw_packqty = pd.read_csv('NPW PackQty.csv', low_memory=False)\\nnpw_packqty['WD'] = 'N'\\nnpw_packqty.set_index(['WD', 'Line Code', 'Part Number'], inplace=True)\\nnpw_packqty\\n\\npw1 = pw.join(other=npw_packqty, on=['WD', 'LC', 'Part Number'], how='left')\\npw1.loc[~pw1['NPW_PackQty'].isna()]\\n\\nf = (pw1['WD']=='N') & (~pw1['NPW_PackQty'].isna())   #Creating the filter for items to be updated\\npw1.loc[f & (pw1['PackQty']!=pw1['NPW_PackQty'])].to_excel('NPW PackQty Not Matching.xlsx')\\n\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "npw_packqty = pd.read_csv('NPW PackQty.csv', low_memory=False)\n",
    "npw_packqty['WD'] = 'N'\n",
    "npw_packqty.set_index(['WD', 'Line Code', 'Part Number'], inplace=True)\n",
    "npw_packqty\n",
    "\n",
    "pw1 = pw.join(other=npw_packqty, on=['WD', 'LC', 'Part Number'], how='left')\n",
    "pw1.loc[~pw1['NPW_PackQty'].isna()]\n",
    "\n",
    "f = (pw1['WD']=='N') & (~pw1['NPW_PackQty'].isna())   #Creating the filter for items to be updated\n",
    "pw1.loc[f & (pw1['PackQty']!=pw1['NPW_PackQty'])].to_excel('NPW PackQty Not Matching.xlsx')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e62bab",
   "metadata": {},
   "source": [
    "OE wheels price MAP fix 11/09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "549e567a-050b-4053-849e-71e4eab0695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##oe_wheel_update = pd.read_excel('OE wheel price increase11-9.xlsx', sheet_name='Sheet1')\n",
    "oe_wheel_update = pd.read_excel('OE Wheel Shipping.xlsx', sheet_name='Sheet1', dtype={'UPC':str})\n",
    "oe_wheel_update.drop_duplicates(subset=['UPC'], inplace=True, keep='last')\n",
    "\n",
    "oe_wheel_update['CS-SKU-NP'] = '2387|' + oe_wheel_update['UPC']\n",
    "oe_wheel_update.set_index('CS-SKU-NP', inplace=True)\n",
    "\n",
    "pw = pw.join(other = oe_wheel_update, on='CS-SKU-NP', rsuffix='_oe')\n",
    "pw.loc[~pw['Shipping (Est)'].isna(), 'Shipping'] = pw.loc[~pw['Shipping (Est)'].isna(), 'Shipping (Est)']\n",
    "pw = pw[pw_cols]   #Restoring Columns\n",
    "del oe_wheel_update   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00826ada-212a-417f-a4e3-c2ad7a66dfee",
   "metadata": {},
   "source": [
    "PA packQty Correction from inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "55b23ce3-2c1e-43fb-ae61-b8b998ffb8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_inv = pd.read_csv('inventory/pa.csv', low_memory=False, encoding= 'unicode_escape')\n",
    "pa_inv['Part'] = pa_inv['Part'].str.replace('-', '', regex=False )\n",
    "pa_inv['Part'] = pa_inv['Part'].str.replace('.', '', regex=False )\n",
    "pa_inv['Part'] = pa_inv['Part'].str.replace('/', '', regex=False )\n",
    "pa_inv['Total_Stock'] = pa_inv[['BxStock', 'ByStock', 'NYStock', 'DCStock', 'AZStock', 'CAStock', 'GAStock']].sum(axis=1)\n",
    "\n",
    "\n",
    "pw = pw.merge(pa_inv, how='left', left_on=['Part Number', 'LC'], right_on=['Part', 'Line'], suffixes=('', 'y'))\n",
    "pw.loc[~pw['Packs'].isna(), 'PackQty'] = pw.loc[~pw['Packs'].isna(), 'Packs']\n",
    "\n",
    "#Manual Fetching the Price and Quantity from PA inventory\n",
    "#pw.loc[~pw['Price'].isna(), 'MinPrice'] = pw.loc[~pw['Price'].isna(), 'Price']\n",
    "#pw.loc[~pw['Price'].isna(), 'item_cost'] = pw.loc[~pw['Price'].isna(), 'Price']\n",
    "#pw.loc[~pw['Total_Stock'].isna(), 'Qty'] = pw.loc[~pw['Total_Stock'].isna(), 'Total_Stock']\n",
    "\n",
    "pw = pw[pw_cols]   #Restoring Columns\n",
    "\n",
    "del pa_inv   #removing variables no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945265d0-08d5-43fc-a9cc-0a594d729723",
   "metadata": {
    "tags": []
   },
   "source": [
    "Force shipping cost and ship markup for RSL Skus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8dc9cad7-32d8-46eb-b29c-3cf2774313e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[(pw['WD'] == '8') & (pw['Shipping'] == 0), ['Shipping', 'ShipMkup']] = 18, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bda58ba2-292b-4b88-adf9-ae7f4f1c2ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preventing Race Sport Lightning items from any other warehouse\n",
    "pw.drop(index=pw.query(\"MasterLC == 329 and WD != '8' \").index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55dc229-9d3c-4f7e-bb8c-291b61b68ff1",
   "metadata": {},
   "source": [
    "Route Dorman Drive Shafts and Dorman Pack Items to Dorman only (remove it from any other warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2ecec1ce-d60a-4496-a0ee-111a7a03e786",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOI = pd.read_csv('Dorman Only Items.csv', low_memory=False, usecols=['MasterSKU']).drop_duplicates()   #Loading the List of items which should only be routed to Dorman\n",
    "pw = pw.merge(right = DOI, how='left', on='MasterSKU', indicator=True)   #Merging the Dorman list to the Price Weight Report\n",
    "pw.drop(pw.query(\"_merge == 'both' and WD!='D'\").index, inplace=True)   #Dropping the DormanDriveShafts and Dorman Pack Items from any other Warehouse Except Dorman\n",
    "pw.drop(columns = '_merge', inplace=True)   #Drop the merging indicator column\n",
    "del DOI   #Delete the list as it is no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-pioneer",
   "metadata": {},
   "source": [
    "#### Calculate markups and format/write price files, by channel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3fbeb-7a53-4100-9ac5-595beb731929",
   "metadata": {},
   "source": [
    "Load Pack SKUs Data from file (to be removed from Price Files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c083d840-1b88-452c-a894-75e90f5997e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "packskus = pd.read_csv('Pack SKUs removed.csv', index_col='MasterSKU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef152355-87a3-4ab2-9592-76655d9adba5",
   "metadata": {},
   "source": [
    "NPW Temporary min Ship Estimate set at 12 (for safe transition to non-flatrate shipping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbfcd70e-2c30-47b6-af88-a16cb31a01b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw.loc[(pw['WD']=='N') & (pw['Shipping']<12), 'Shipping'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ae96545-fa43-443d-b0e6-252c051d7e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_channel(channel):    \n",
    "    pf = pw.copy()\n",
    "\n",
    "    #Cleaning motorstate data - only retaining ALLSTARPERFORMANCE(261) or SKUs that don't contain punctuation and non-zero price\n",
    "    pf['WC'] = pf.groupby(['MasterSKU'])['WD'].transform('nunique')   #Adding column for count of Warehouses in which each MasterSKU is present\n",
    "    m = pf.query(\" WD=='Y' and MasterLC!=261 and ( `Part Number`.str.contains('-',regex=False) or `Part Number`.str.contains('.',regex=False) or `Part Number`.str.contains('/',regex=False) or `Part Number`.str.contains('(',regex=False) or `Part Number`.str.contains(')',regex=False) or MinPrice==0 or WC==1)\").index\n",
    "    pf.drop(index=m, inplace=True, columns='WC')\n",
    "    \n",
    "    #Removing Speicfic SKU(s) from Price Files\n",
    "    #SKU 1 P650|W41029 : Marion said on 30th September its Refunded 6 times so remove from price file \n",
    "    pf = pf.loc[pf['CS-SKU-NP']!= \"P650|W41029\"]\n",
    "    \n",
    "    if 'Ebay' in channel['name']:\n",
    "    #Excluding brands due to eBay Violation\n",
    "    \n",
    "        #Excluding AFE\n",
    "        pf = pf.loc[pf['MasterLC']!=510]\n",
    "\n",
    "        #Excluding Jet Chips\n",
    "        pf = pf.loc[pf['MasterLC']!=161]\n",
    "\n",
    "        #Excluding Russel\n",
    "        pf = pf.loc[pf['MasterLC']!=265]\n",
    "        \n",
    "        #Excluding Diablo\n",
    "        pf = pf.loc[pf['MasterLC']!=590]\n",
    "        \n",
    "        #Excluding SCT Performance\n",
    "        pf = pf.loc[pf['MasterLC']!=837]\n",
    "        \n",
    "        #Excluding Holley\n",
    "        pf = pf.loc[pf['MasterLC']!=453]\n",
    "        \n",
    "        #Removing the Holley Carburetor Repair Kit and Magnaflow Exhaust System Kits\n",
    "        eBay_Violation_skus = pd.read_csv('EBay Violation SKUs.csv', low_memory = False, usecols=['SKU'])        \n",
    "        pf = pf.merge(eBay_Violation_skus, how='left', left_on='MasterSKU', right_on='SKU')\n",
    "        pf = pf.loc[pf['SKU'].isnull()]\n",
    "        pf.drop(columns = 'SKU', inplace = True)\n",
    "\n",
    "        \n",
    "\n",
    "    # Filter price file if APF or Mecka\n",
    "    if channel['name'] == 'AP Fusion':\n",
    "        pf.loc[pf['WD']!='Z', ['MinQty','MaxQty']] = 4, 12   ## Changed Min to four from three\n",
    "        apf_csskus = pd.read_csv('apf-skus.csv')['cssku']\n",
    "        apf_csskus = apf_csskus.drop_duplicates()\n",
    "        pf = pf[pf['CS-SKU-NP'].str[1:].isin(apf_csskus)]\n",
    "    if channel['name'] == 'Mecka':\n",
    "        pf = pf[(pf['WD']=='D') & (pf['item_cost'] >= 30)]\n",
    "    # FIlter BS Walmart \n",
    "    if channel['name'] == 'BS Walmart':\n",
    "        pf.drop(pf.loc[pf['WD'] == 'Y'].index, inplace=True)\n",
    "        ##pf.drop(pf.loc[pf['WD'] == 'P'].index, inplace=True)\n",
    "        pf.drop(pf.loc[pf['WD'] == 'J'].index, inplace=True)\n",
    "        pf.drop(pf.loc[pf['WD'] == 'C'].index, inplace=True)\n",
    "        pf.drop(pf.loc[pf['WD'] == '8'].index, inplace=True)        \n",
    "\n",
    "\n",
    "    # # Calculate markups\n",
    "    target_profit = channel['target_profit'] if 'target_profit' in channel else DEFAULT_TARGET_PROFIT\n",
    "    channel_fees = channel['channel_transaction_fee'] + CS_TRANSACTION_FEE\n",
    "\n",
    "    pf['our_cost'] = np.nan\n",
    "    pf['our_markup'] = np.nan\n",
    "\n",
    "    bfilter = pf['WD']=='C'\n",
    "    pf.loc[bfilter, 'our_cost'] = pf.loc[bfilter, 'item_cost']\n",
    "\n",
    "    # Handling Brock\n",
    "    #+ $6 for any item costing < $50              \n",
    "    pf.loc[bfilter & (pf['item_cost']<=50), 'our_cost' ] = pf.loc[bfilter & (pf['item_cost']<=50), 'item_cost' ] + 4\n",
    "    # # 10% marup for all items, as buffer against no-return policy (adjusted in new Target profit = TP*1.1+0.1\n",
    "    #pf.loc[bfilter, 'our_cost' ] = pf.loc[bfilter, 'our_cost' ] * 1.1\n",
    "    \n",
    "    #Testing implementation of independant Shipping and cost markups for Brock    \n",
    "    pf.loc[bfilter, 'TP'] = (1 - channel_fees) * (1/pf['ShipMkup'] - 1) + target_profit   #calculating equivalent target profit to reach at same/ current final price before this implementation\n",
    "    pf.loc[bfilter, 'our_markup'] = pf.loc[bfilter, 'our_cost'] / ( (1 + target_profit*1.1+0.1) * (pf.loc[bfilter, 'our_cost']) / (1 - channel_fees) )\n",
    "    pf.loc[bfilter, 'ShipMkup'] = pf.loc[bfilter, 'Shipping'] / ( (1 + pf.loc[bfilter, 'TP']) * (pf.loc[bfilter, 'Shipping']) / (1 - channel_fees) )        \n",
    "    pf.loc[((bfilter) & (pf['Shipping']==0)), 'ShipMkup'] = 1\n",
    "    \n",
    "    \n",
    "    # then adjust to find what the equivalent markup would be (to get to same final price) with the original cost value\n",
    "    pf.loc[bfilter, 'Markup'] = pf.loc[bfilter, 'item_cost'] * pf.loc[bfilter, 'our_markup'] / pf.loc[bfilter, 'our_cost']\n",
    "    pf.drop(columns=['our_cost', 'our_markup'], inplace=True)\n",
    "\n",
    "    # handle Jante, certain skus specially and removing retaining only selected SKUs for PS Ebay\n",
    "        \n",
    "    if ( channel['name'] == 'PS Ebay'):        \n",
    "        f_1367 = pf['CS-SKU-NP'].str[:4]=='1367'\n",
    "        pf.loc[f_1367, 'Markup'] = pf.loc[f_1367, 'item_cost'] / ( (1 + target_profit) * (pf.loc[f_1367, 'item_cost'] + pf.loc[f_1367, 'Shipping']) / (1 - channel_fees) - pf.loc[f_1367, 'Shipping'])\n",
    "\n",
    "        # Added 1% more to channel fees  ---------------- this will never execute due to logical error - it will only execute if above condition is false which will never happen because 367 is the only line Jante has\n",
    "        f_jante_n1367 = (pf['CS-SKU-NP'].str[:4]!='1367') & (pf['CS-SKU-NP'].str[0]=='1')\n",
    "        pf.loc[f_jante_n1367, 'Markup'] = pf.loc[f_jante_n1367, 'item_cost'] / ( (1 + target_profit) * (pf.loc[f_jante_n1367, 'item_cost'] + pf.loc[f_jante_n1367, 'Shipping']) / (1 - (channel_fees + 0.01)) - pf.loc[f_jante_n1367, 'Shipping'])\n",
    "\n",
    "        PS_Ebay_F = pd.read_csv('PS Ebay Listings.csv', low_memory=False)   #Retaining only approved SKUs for PS Ebay\n",
    "        pf = pf.merge(PS_Ebay_F, how='inner', left_on='MasterSKU', right_on='MasterSKU')\n",
    "        pf['CatSKU'] = pf['Cat']\n",
    "        pf.drop(columns='Cat', inplace=True)\n",
    "        pf.loc[pf['CatSKU']=='Y', 'CS-SKU-NP'] = pf.loc[pf['CatSKU']=='Y', 'CS-SKU-NP-CatSKU']\n",
    "    else:\n",
    "        pf['CatSKU'] = 'N'        \n",
    "        \n",
    "\n",
    "    # Dorman increase profit\n",
    "    f_dorman = pf['CS-SKU-NP'].str[0]=='D'   #Defining Filter for Dorman\n",
    "    \n",
    "    pf.loc[f_dorman, 'Markup']  = pf.loc[f_dorman, 'item_cost'] / ( (1 + 0.06) * (pf.loc[f_dorman, 'item_cost'] + pf.loc[f_dorman, 'Shipping']) / (1 - channel_fees) - pf.loc[f_dorman, 'Shipping'])\n",
    "    g_300 = pf['item_cost'] >= 300\n",
    "    pf.loc[f_dorman & g_300, 'Markup'] = pf.loc[f_dorman & g_300, 'item_cost'] / ( (1 + 0.0575) * (pf.loc[f_dorman & g_300, 'item_cost'] + pf.loc[f_dorman & g_300, 'Shipping']) / (1 - channel_fees) - pf.loc[f_dorman & g_300, 'Shipping'])\n",
    "    g_500 = pf['item_cost'] >= 500\n",
    "    pf.loc[f_dorman & g_500, 'Markup'] = pf.loc[f_dorman & g_500, 'item_cost'] / ( (1 + 0.055) * (pf.loc[f_dorman & g_500, 'item_cost'] + pf.loc[f_dorman & g_500, 'Shipping']) / (1 - channel_fees) - pf.loc[f_dorman & g_500, 'Shipping'])\n",
    "    g_1000 = pf['item_cost'] >= 1000\n",
    "    pf.loc[f_dorman & g_1000, 'Markup'] = pf.loc[f_dorman & g_1000, 'item_cost'] / ( (1 + 0.05) * (pf.loc[f_dorman & g_1000, 'item_cost'] + pf.loc[f_dorman & g_1000, 'Shipping']) / (1 - channel_fees) - pf.loc[f_dorman & g_1000, 'Shipping'])\n",
    "    \n",
    "    # base case, all other skus\n",
    "    f_rem = pf['Markup'].isna()   #Defining Filter for remaining Data\n",
    "    \n",
    "    if channel['name'] == 'AP Fusion':\n",
    "        pf.loc[f_rem, 'ShipMkup'] = 0.95\n",
    "        pf['Shipmarkedup'] = pf['Shipping'] / pf['ShipMkup']\n",
    "        pf.loc[f_rem, 'Markup'] = pf.loc[f_rem, 'item_cost'] / ( (1 + target_profit) * (pf.loc[f_rem, 'item_cost'] ) / (1 - channel_fees) + (pf.loc[f_rem, 'Shipmarkedup']/(1 - channel_fees) - pf.loc[f_rem, 'Shipmarkedup']))        \n",
    "        pf.drop (columns='Shipmarkedup', inplace=True)\n",
    "        \n",
    "        #Testing implementation of independant Shipping and cost markups for all other than Brock\n",
    "        pa_filter = ((pf['WD']=='Z') | (pf['WD']=='P') | (pf['WD']=='N') | (pf['WD']=='K') | (pf['WD']=='Y') | (pf['WD']=='2') | (pf['WD']=='6') | (pf['WD']=='9') | (pf['WD']=='1') | (pf['WD']=='8') | (pf['WD']=='J') )        \n",
    "        pf.loc[pa_filter, 'TP'] = (1 - channel_fees) * (1/(0.95*0.95) - 1) + 0   #calculating equivalent target profit to reach at same/ current final price before this implementation\n",
    "        pf.loc[pa_filter, 'Markup'] = pf.loc[pa_filter, 'item_cost'] / ( (1 + target_profit) * (pf.loc[pa_filter, 'item_cost']) / (1 - channel_fees) )\n",
    "        pf.loc[pa_filter, 'ShipMkup'] = pf.loc[pa_filter, 'Shipping'] / ( (1 + pf.loc[pa_filter, 'TP']) * (pf.loc[pa_filter, 'Shipping']) / (1 - channel_fees) )        \n",
    "        pf.loc[((pa_filter) & (pf['Shipping']==0)), 'ShipMkup'] = 1      \n",
    "        \n",
    "        #Decreasing PFG item and Shipping Target profits to 8% for APFusion\n",
    "        pfg_filter = (pf['WD']=='J')   \n",
    "        \n",
    "        pf.loc[pfg_filter, 'Markup'] =   (1 - channel_fees) / (1 + 0.08)\n",
    "        pf.loc[pfg_filter, 'ShipMkup'] = (1 - channel_fees) / (1 + 0.08)\n",
    " \n",
    "        #Testing implementation of independant Shipping and cost markups for Dorman\n",
    "        d_filter = ((pf['WD']=='D') & (pf['item_cost']<=30 ))\n",
    "        pf.loc[d_filter, 'TP'] = (1 - channel_fees) * (1/pf['ShipMkup'] - 1) + 0.06   #calculating equivalent target profit to reach at same/ current final price before this implementation\n",
    "        pf.loc[d_filter, 'Markup'] = pf.loc[d_filter, 'item_cost'] / ( (1 + 0.06) * (pf.loc[d_filter, 'item_cost']) / (1 - channel_fees) )\n",
    "        pf.loc[d_filter, 'ShipMkup'] = pf.loc[d_filter, 'Shipping'] / ( (1 + pf.loc[d_filter, 'TP']) * (pf.loc[d_filter, 'Shipping']) / (1 - channel_fees) )\n",
    "        pf.loc[((d_filter) & (pf['Shipping']==0)), 'ShipMkup'] = 1\n",
    "        \n",
    "    else:\n",
    "\n",
    "        pf.loc[f_rem, 'Markup'] = pf.loc[f_rem, 'item_cost'] / ( (1 + target_profit) * (pf.loc[f_rem, 'item_cost'] + pf.loc[f_rem, 'Shipping']) / (1 - channel_fees) - pf.loc[f_rem, 'Shipping'])        \n",
    "        \n",
    "        #Testing implementation of independant Shipping and cost markups for all other than Brock\n",
    "        pa_filter = ((pf['WD']=='Z') | (pf['WD']=='P') | (pf['WD']=='N') | (pf['WD']=='K') | (pf['WD']=='Y') | (pf['WD']=='2') | (pf['WD']=='6') | (pf['WD']=='9') | (pf['WD']=='1') | (pf['WD']=='8') | (pf['WD']=='J') )\n",
    "        pf.loc[pa_filter, 'TP'] = (1 - channel_fees) * (1/pf['ShipMkup'] - 1) + target_profit   #calculating equivalent target profit to reach at same/ current final price before this implementation\n",
    "        pf.loc[pa_filter, 'Markup'] = pf.loc[pa_filter, 'item_cost'] / ( (1 + target_profit) * (pf.loc[pa_filter, 'item_cost']) / (1 - channel_fees) )\n",
    "        pf.loc[pa_filter, 'ShipMkup'] = pf.loc[pa_filter, 'Shipping'] / ( (1 + pf.loc[pa_filter, 'TP']) * (pf.loc[pa_filter, 'Shipping']) / (1 - channel_fees) )        \n",
    "        pf.loc[((pa_filter) & (pf['Shipping']==0)), 'ShipMkup'] = 1\n",
    "\n",
    "        #Testing implementation of independant Shipping and cost markups for Dorman\n",
    "        d_filter = ((pf['WD']=='D') & (pf['item_cost']<=30 ))\n",
    "        pf.loc[d_filter, 'TP'] = (1 - channel_fees) * (1/pf['ShipMkup'] - 1) + 0.06   #calculating equivalent target profit to reach at same/ current final price before this implementation\n",
    "        pf.loc[d_filter, 'Markup'] = pf.loc[d_filter, 'item_cost'] / ( (1 + 0.06) * (pf.loc[d_filter, 'item_cost']) / (1 - channel_fees) )\n",
    "        pf.loc[d_filter, 'ShipMkup'] = pf.loc[d_filter, 'Shipping'] / ( (1 + pf.loc[d_filter, 'TP']) * (pf.loc[d_filter, 'Shipping']) / (1 - channel_fees) )\n",
    "        pf.loc[((d_filter) & (pf['Shipping']==0)), 'ShipMkup'] = 1\n",
    "        \n",
    "        #Removing SKUs with Pack Quantities (except Dorman Pack SKUs)\n",
    "        pf = pf.merge(right = packskus, on='MasterSKU', how='left', indicator=True)\n",
    "        pf.drop ( index = pf.loc[pf['_merge']=='both'].index, columns='_merge', inplace=True)\n",
    "\n",
    "    \n",
    "    # Clean up\n",
    "    pf.loc[pf['MinPrice'] < 1,'MinPrice'] = 1\n",
    "    pf.loc[pf['Markup'] < .1,'Markup'] = .1\n",
    "    pf.loc[pf['Markup'] > 1,'Markup'] = 1\n",
    "    pf['Markup'] = pf['Markup'].round(3)\n",
    "    pf['ShipMkup'] = pf['ShipMkup'].round(3)\n",
    "    pf['Shipping'] = pf['Shipping'].round(2)\n",
    "    pf.loc[pf['Shipping'].lt(0), 'Shipping'] = 0    \n",
    "    #pf['total_cost'] = pf['item_cost'] + pf['Shipping'] # hoping this will fix most examples of Dorman going thru PA    \n",
    "    pf['fprice'] = (pf['item_cost'] * pf['PackQty']) / pf['Markup'] + pf['Shipping']/pf['ShipMkup']   #Computing the Final Total cost - For Future Use\n",
    "    \n",
    "\n",
    "    pfl = []\n",
    "    pfl.append(pf.query(\"WD == 'D' and Qty>0 \"))   #Dorman preference - route any parts available in Dorman to Dorman\n",
    "    pfl.append(pf.query(\"MasterLC == 308 and WD == 'K' and Qty>0\").sort_values(['fprice', 'Qty'], ascending=[True, False]))   #Prefer Walker Exhaust to be routed through Keystone if available\n",
    "    \n",
    "    qty_threshold = 5           \n",
    "    #Prefer Lowest final price for items with available quantity >= threshold (5)    (except NPW)\n",
    "    pfl.append(pf.query(\"Qty >= @qty_threshold and WD!='N'\") .sort_values(['fprice', 'Qty'], ascending=[True,False])\\\n",
    "           .drop_duplicates(subset=['MasterSKU'], keep='first'))\n",
    "    \n",
    "    #Putting in all NPW parts with Quantity >= threshold (5) so that NPW will be applicable if quantity in other warehouses is less than threshold\n",
    "    pfl.append(pf.query(\"Qty >= @qty_threshold and WD=='N'\"))\n",
    "       \n",
    "    \n",
    "    #Prefer Highest available quantity for any items with available quantity below threshold\n",
    "    pfl.append(pf.query(\"Qty>0 and Qty < @qty_threshold and WD!='N'\").sort_values(['Qty','fprice'], ascending=[False,True])\\\n",
    "           .drop_duplicates(subset=['MasterSKU'], keep='first'))\n",
    "    \n",
    "    pfl.append(pf.query(\"Qty>0 and Qty < @qty_threshold and WD=='N'\"))\n",
    "    \n",
    "    pfl.append(pf.query(\"Qty==0\").sort_values(['Qty','fprice'], ascending=[False,True])\\\n",
    "       .drop_duplicates(subset=['MasterSKU'], keep='first'))\n",
    "    \n",
    "    pf = pd.concat(pfl)\n",
    "\n",
    "    pf = pf.drop_duplicates (subset=['MasterSKU'], keep='first')  \n",
    "    \n",
    "    # Return the Price File\n",
    "    return pf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1f84ae1-17e0-40b2-bab0-b7049512aeed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'AP Fusion', 'channel_transaction_fee': 0.08, 'target_profit': 0.09} 10.051\n",
      "{'name': 'PS Amazon', 'channel_transaction_fee': 0.119} 28.873\n",
      "{'name': 'PS Walmart', 'channel_transaction_fee': 0.125} 25.61\n",
      "{'name': 'PS Ebay', 'channel_transaction_fee': 0.17} 19.802\n",
      "{'name': 'BS Amazon', 'channel_transaction_fee': 0.12} 30.129\n",
      "{'name': 'BS Walmart', 'channel_transaction_fee': 0.12} 29.362\n",
      "{'name': 'BS Ebay', 'channel_transaction_fee': 0.12} 33.206\n",
      "{'name': 'Mecka', 'channel_transaction_fee': 0.17} 5.702\n",
      "Total Time Taken :  248.22282075881958  seconds\n"
     ]
    }
   ],
   "source": [
    "sm = []   #For extraction of shipping Target Profits\n",
    "for c in CHANNELS:\n",
    "    t1 = time.time()\n",
    "    pf = process_channel(c)   #Processing the channel and getting Price File\n",
    "    \n",
    "    s = pf.groupby(['TP', 'WD']).size().reset_index(name='count')\n",
    "    s['name'] = c['name']\n",
    "    sm.append(s)\n",
    "    \n",
    "    pf = pf[PRICE_FILE_COLUMNS]   #Only taking the columns needed for the Price File\n",
    "    \n",
    "    # Write price file to Disk\n",
    "    pf.to_csv(f\"{PRICE_FILE_LOCATION}/{c['name']}.csv\", index=False)\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print (c, round(t2-t1,3))\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print ('Total Time Taken : ', end_time - start_time, ' seconds')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1603d0be3227fab8159d0092641500a55c974a65b8b6791f95fc8c8f1af5acb6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
